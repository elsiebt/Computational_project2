---
title: "TMA4300 Computer Intensive Statistical Methods"
subtitle: "Exercise 2"
author: "Maja B. Mathiassen & Elsie B. Tandberg"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(boot)
library(ggplot2)
library(invgamma)
```

# Problem A: The coal mining disaster data
In this problem we look at a data set of mining disasters in the UK, from March 15th 1951 to March 22nd 1962.

## A1

To get an impression of the data set, we make a plot of the cumulative number of disasters as a function of time. 
```{r A1, fig.cap="Caption"}
#The x-axis contains the specific time of an accident
x <- coal$date

#y is the cumulative sum of disasters
y <- rep(1,length(x))
y[1]<-0
y <- cumsum(y)
y[length(x)]<-0

#Plotting
ggplot()+
  geom_line(aes(x=x[1:190],y=y[1:190])) +
  xlab('Time') + ylab('Cumulative number of disasters') + 
  ggtitle('Mining disasters') 
```
From the figure above we can see that ..... etc. 

## A2

We denote the starting time of the observations as $t_0$ and the end time as $t_2$. We assume the coal mining disasters follow an inhomogeneous Poisson process with intensity function

$$
\begin{aligned}
\lambda(t)=\begin{cases}
\lambda_0, \quad t\in[t_0, t_1), \\
\lambda_1, \quad t\in[t_1, t_2],
\end{cases}
\end{aligned}
$$ 

where $t_1\in(t_0, t_2)$. The likelihood function for the observed data becomes

$$
\begin{aligned}
f(x|t_1,\lambda_0,\lambda_1) = \exp\{-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)\}\lambda_0^{y_0}\lambda_1^{y_1},
\end{aligned}
$$

where $y_0$ is the number of mining disasters from $t_0$ to $t_1$, and $y_1$ is the number of disasters from $t_1$ to $t_2$. We assume $t_1\sim\text{Unif}(t_0,t_2)$ and that $t_1$, $\lambda_0$ and $\lambda_1$ all are apriori independent of each other. 

We use a gamma distributed prior for $\lambda_0$ and $\lambda_1$, with $\alpha=2$ and scale parameter $\beta$. Then

$$
\begin{aligned}
f(\lambda_i|\beta)=\frac{1}{\beta^2}\lambda_i\exp\{-\frac{\lambda_0}{\beta}\}, \quad i=0,1.
\end{aligned}
$$

For $\beta$, we use the improper prior 

$$
\begin{aligned}
f(\beta)\propto\frac{\exp\{-1/\beta\}}{\beta},
\end{aligned}
$$

for some $\beta>0$.

The parameters in our model are $\theta=(t_1,\lambda_0,\lambda_1,\beta)$. The posterior distribution of $\theta$ is given by

$$
\begin{aligned}
f(\theta|x)&=f(x|t_1, \lambda_0, \lambda_1,\beta)f(t_1)f(\lambda_0|\beta)f(\lambda_1|\beta)f(\beta)\\
&\propto\exp\{-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)\}\lambda_0^{y_0}\lambda_1^{y_1}\frac{1}{\beta^5}\lambda_0\lambda_1\exp\{-\frac{1}{\beta}(\lambda_0+\lambda_1)-\frac{1}{\beta}\}.
\end{aligned}
$$

## A3
The full conditionals are

$$
\begin{aligned}
f(t_1|...) &\propto \exp\{-t_1(\lambda_0-\lambda_1)\}\lambda_0^{y_0}\lambda_1^{y_1} \\
f(\lambda_0|...) &\propto \lambda_0^{y_0+1} \exp\{-\lambda_0(\frac{1}{\beta} +t_1-t_0)\} \\
f(\lambda_1|...) &\propto \lambda_1^{y_1+1} \exp\{-\lambda_1(\frac{1}{\beta} +t_2-t_1)\} \\
f(\beta|...) &\propto \frac{1}{\beta^5}\exp\{\frac{1}{\beta}(1+\lambda_0+\lambda_1)\}
\end{aligned}
$$


From this we can see that $f(t_1|...)$ does not belong to a distribution. Next $f(\lambda_0|...)\sim\text{Gamma}(\alpha_0,\beta_0)$ when $\alpha_0=y_0+2$ and $\beta_0^{-1}=1/\beta + t_1 - t_0$, and $f(\lambda_1|...)\sim\text{Gamma}(\alpha_1,\beta_1)$ when $\alpha_1=y_1+2$ and $\beta_1^{-1}=1/\beta + t_2 - t_1$. Lastly $f(\beta|...)\sim \text{invGamma}(4, 1+\lambda_0 + \lambda_1)$.

## A4

We want to implement a single site MCMC algorithm for $f(\theta|x)$. Since $\lambda_0$, $\lambda_1$, and $\beta$ are from known distributions we can use Gibbs sampling for these variables. For $t_1$ we choose to use random walk, and 
$$
\begin{aligned}
t_1^{i}\sim \mathcal{N}(t_1^{i-1}, \sigma^2),
\end{aligned}
$$
where $t_1^{i-1}$ is the current value and $t_1^i$ is the proposed new value. The acceptance probability becomes
$$
\begin{aligned}
\alpha = \min\bigg(1, \frac{f(t_1^i|...)}{f(t_1^{i-1}|...)}\bigg) = \min\Big(1, \exp\{(t_1^{i-1}-t_1^i)(\lambda_0-\lambda_1)\}\lambda_0\lambda_1\Big)
\end{aligned}
$$
maybe... I am not entirely sure...

```{r}
find_y0_y1 = function(t0, t1, t2) {
  if (coal$date[190] < t1) { # If t1 is after the last disaster 
    y0 = 189        
  } else { # Find out how many disasters happened before t1
    y0 = 0
    i = 2
    done = FALSE
    while (done == F) {
      if (coal$date[i] < t1) {
        i = i+1
      } else {
        done = TRUE
        i = i-1
      }
    }
    y0 = coal$cumulative[i]
  }
  y1 = 189 - y0 # How many disasters happened after t1
  return( c(y0,y1) )
}
```


```{r}
#Trying to make the algorithm
theta0 <- c(1900, 1, 1, 3)
t0 <- 1851
t2 <- 1962
MCMC <- function(theta, n, sigma){
  t1 <- rep(0, n)
  t1[1]<-theta[1]
  lambda0 <- rep(0,n)
  lambda0[1] <- theta[2]
  lambda1 <- rep(0,n)
  lambda1[1]<-theta[3]
  beta <- rep(0,n)
  beta[1]<-theta[4]
  for(i in 2:n){
     #Sampling theta1 using Gibbs
      y0 <- sum(coal$date<t1[i-1])-1
      y1 <- 189-y0
      lambda0[i] <- rgamma(1, y0+2, 1/(t1[i-1]-t0+1/beta[i-1]))
      lambda1[i] <- rgamma(1, y1+2, 1/(t2-t1[i-1]+1/beta[i-1]))
      beta[i] <- rinvgamma(1, 4, 1+lambda0[i-1]+lambda1[i-1])
      
      
      #Metropolis-Hastings for sampling from t1
      proposal <-rnorm(1, t1[i-1], sigma)
      y0.prop <- sum(coal$date<proposal)-1
      y1.prop <- 189-y0.prop
      if(proposal<t0 || proposal>t2){
        accept.prob=0
      }
      else{
        #Finding the acceptance probability
        numerator.log <- -proposal*(lambda0[i]-lambda1[i])+y0.prop*log(lambda0[i])+y1.prop*log(lambda1[i])
        denominator.log <- -t1[i-1]*(lambda0[i]-lambda1[i])+y0*log(lambda0[i])+y1*log(lambda1[i])

        accept.prob <- min(1, exp(numerator.log-denominator.log))
      }
      u <- runif(1)
      if(u<accept.prob){
        t1[i]<-proposal
      }
      else{
        t1[i]<-t1[i-1]
      }
      
      }
  thetamat <- data.frame(t1,lambda0, lambda1, beta)
  return(thetamat)
}

simulation = MCMC(theta0,2000,3)
```

## A5

``` {r A5}
n = length(t(simulation[1]))

# Trace plot of t1
ggplot() +
  geom_line( aes(x = 1:n, y = t(simulation[1])) ) +
  xlab('Iterations') + ylab(expression(t[1])) + 
  ggtitle(expression(paste('Trace plot of ', t[1]))) 

# Trace plot of lambda0
ggplot()+
  geom_line( aes(x = 1:n, y = t(simulation[2])) ) +
  xlab('Iterations') + ylab(expression(lambda[0])) + 
  ggtitle(expression(paste('Trace plot of ', lambda[0])))

# Trace plot of lambda1
ggplot()+
  geom_line( aes(x = 1:n, y = t(simulation[3])) ) +
  xlab('Iterations') + ylab(expression(lambda[1])) + 
  ggtitle(expression(paste('Trace plot of ', lambda[1])))

# Trace plot of beta
ggplot()+
  geom_line( aes(x = 1:n, y = t(simulation[4])) ) +
  xlab('Iterations') + ylab(expression(beta)) + 
<<<<<<< HEAD
  ggtitle(expression(paste('Trace plot of ', beta)))
```

```{r}
par(mfrow = c(2,2))
  ggtitle(expression(paste('Trace plot of ', beta))) 

```
From the trace plots it seems likely that the parameters have converged. The values have stabilized after 250 iterations so this is the burn-in period for all the four parameters. 

The mixing properties are evaluated by looking at the autocorrelation plots. 

```{r}
par(mfrow = c(2, 2))

#Mixing properties
acf(simulation[1])
acf(simulation[2])
acf(simulation[3])
acf(simulation[4])
```
The autocorrelations plots for  $\t_1$, $\lambda_0$ and $\lambda_1$ are similar and so the mixing properties 

## 6
Our algorithm has a tuning parameter $\sigma$ used to sample $t_1$ from the normal distribution. We will now explore how its value affect the burn-in and mixing properties. 

```{r}
burn.in <- function(sigma){
  simulation = MCMC(theta0,2000,sigma)

  n = length(t(simulation[2]))
  #par(mfrow = c(2, 2))
    # Trace plot of t1
    ggplot() +
      geom_line( aes(x = 1:n, y = t(simulation[1])) ) +
      xlab('Iterations') + ylab(expression(t[1])) + 
      ggtitle(expression(paste('Trace plot of ', t[1]))) 
    
    # Trace plot of lambda0
    ggplot()+
      geom_line( aes(x = 1:n, y = t(simulation[2])) ) +
      xlab('Iterations') + ylab(expression(lambda[0])) + 
      ggtitle(expression(paste('Trace plot of ', lambda[0])))
    
    # Trace plot of lambda1
    ggplot()+
      geom_line( aes(x = 1:n, y = t(simulation[3])) ) +
      xlab('Iterations') + ylab(expression(lambda[1])) + 
      ggtitle(expression(paste('Trace plot of ', lambda[1])))
    
    # Trace plot of beta
    ggplot()+
      geom_line( aes(x = 1:n, y = t(simulation[4])) ) +
      xlab('Iterations') + ylab(expression(beta)) + 
      ggtitle(expression(paste('Trace plot of ', beta))) 

}

burn.in(2)
```


## 7
We will now implement a block Metropolis Hastings algortihm using two block proposals. The first proposal for $(t_1, \lambda_0, \lambda_1)$ where $\beta$ is kept unchanged. Proposal values $(\tilde t_1, \tilde \lambda_0, \tilde \lambda_1)$ will be generated by first generating $(\tilde t_1$ from the normal distribution centered at the current value of $t_1$, which was done in A4. And then generating $\tilde \lambda_0, \tilde \lambda_1)$ with the new value of $(\tilde t_1$. The acceptance probability is now changed so that it is now dependent on all the three parameters. Resulting in the following acceptance probability on log scale.

$$
\alpha=\frac{\exp(- \tilde \lambda_0(\tilde t_1-t_0)-\tilde \lambda_1(t_2-\tilde t_1)+(\tilde y_0+1)\log\tilde \lambda_0+(\tilde y_1+1)\log\tilde \lambda_1-\frac{1}{\beta}(\tilde \lambda_0+ \tilde \lambda_1))}{\exp(- \lambda_0( t_1-t_0)- \lambda_1(t_2- t_1)+(y_0+1)\log \lambda_0+(y_1+1)\log \lambda_1-\frac{1}{\beta}( \lambda_0+  \lambda_1))}
$$
When $t_1$ is kept constant we have 

$$
\alpha=\frac{\exp(- \tilde \lambda_0( t_1-t_0)-\tilde \lambda_1(t_2- t_1)+( y_0+1)\log\tilde \lambda_0+( y_1+1)\log\tilde \lambda_1-5\log\tilde \beta-\frac{1}{\tilde \beta}(\tilde \lambda_0+ \tilde \lambda_1)-\frac{1}{\tilde \beta}}{\exp(- \lambda_0( t_1-t_0)- \lambda_1(t_2- t_1)+(y_0+1)\log \lambda_0+(y_1+1)\log \lambda_1-5\log \beta-\frac{1}{\beta}( \lambda_0+  \lambda_1)-\frac{1}{\beta}}
$$

```{r}
#Making MCMC with block
theta0 <- c(1900, 1, 1, 3)
t0 <- 1851
t2 <- 1962

block1 <- function(t1, lambda0, lambda1, beta, sigma.t1){
      #Sampling theta using Gibbs
      y0 <- sum(coal$date<t1)-1
      y1 <- 189-y0

      
      #Metropolis-Hastings for sampling from t1
      t1.prop <-rnorm(1, t1, sigma.t1) #Generating proposal value for t1 from normal distribution
      y0.prop <- sum(coal$date<t1.prop)-1
      y1.prop <- 189-y0.prop
      lambda0.prop <- rgamma(1, y0.prop+2, 1/(t1.prop-t0+1/beta)) 
      lambda1.prop <- rgamma(1, y1.prop+2, 1/(t2-t1.prop+1/beta))
      
      print(t1.prop)
      
      if(t1.prop<t0 || t1.prop>t2){
        accept.prob=0
      }
      else{
        #Finding the acceptance probability
        numerator.log <- -lambda0.prop*(t1.prop-t0)-lambda1.prop*(t2-t1.prop)+(y0.prop+1)*log(lambda0.prop)+(y1.prop+1)*log(lambda1.prop)-1/beta*(lambda0.prop+lambda1.prop)
        denominator.log <- -lambda0*(t1-t0)-lambda1*(t2-t1)+(y0+1)*log(lambda0)+(y1+1)*log(lambda1)-1/beta*(lambda0+lambda1)

        accept.prob <- min(1, exp(numerator.log-denominator.log))
        print(numerator.log-denominator.log)
      }
      u <- runif(1)
      if(u<accept.prob){
        #accept
        t1.update<-t1.prop
        lambda0.update<-lambda0.prop
        lambda1.update<-lambda1.prop
      }
      else{
        #reject, keeping old values
        t1.update<-t1
        lambda0.update<-lambda0
        lambda1.update<-lambda1
      }
      
      return(c(t1.update, lambda0.update, lambda1.update, beta))
}

block1(theta0[1], theta0[2], theta0[3], theta0[4],20)
#2%%2
```

```{r}
block2 <- function(t1, lambda0, lambda1, beta, sigma.beta){
      #Sampling theta using Gibbs
      y0 <- sum(coal$date<t1)-1
      y1 <- 189-y0

      
      #Metropolis-Hastings for sampling from t1
      beta.prop <-rnorm(1, beta, sigma.beta) #Generating proposal value for t1 from normal distribution
      lambda0.prop <- rgamma(1, y0+2, 1/(t1-t0+1/beta.prop)) 
      lambda1.prop <- rgamma(1, y1+2, 1/(t2-t1+1/beta.prop))
      
      
      #Finding the acceptance probability
      numerator.log <- -lambda0.prop*(t1-t0)-lambda1.prop*(t2-t1)+(y0+1)*log(lambda0.prop)+(y1+1)*log(lambda1.prop)-5*log(beta.prop)-1/beta.prop*(lambda0.prop+lambda1.prop)-1/beta.prop
      denominator.log <- -lambda0*(t1-t0)-lambda1*(t2-t1)+(y0+1)*log(lambda0)+(y1+1)*log(lambda1)-5*log(beta)-1/beta*(lambda0+lambda1)-1/beta

        accept.prob <- min(1, exp(numerator.log-denominator.log))
      
      u <- runif(1)
      if(u<accept.prob){
        #accept
        lambda0<-lambda0.prop
        lambda1<-lambda1.prop
        beta <- beta.prop
      }
      else{
        #reject, keeping old values
        lambda0<-lambda0
        lambda1<-lambda1
        beta<- beta
      }
      
      return(c(t1, lambda0, lambda1, beta))
}
block2(theta0[1], theta0[2], theta0[3], theta0[4],10)


```


```{r}
MCMC_block <- function(theta, n, sigma.t1, sigma.beta){
  #Setting the initial values
  t1 <- rep(0, n)
  t1[1]<-theta[1]
  lambda0 <- rep(0,n)
  lambda0[1] <- theta[2]
  lambda1 <- rep(0,n)
  lambda1[1]<-theta[3]
  beta <- rep(0,n)
  beta[1]<-theta[4]
  
  #Generating n samples of the parameters
  for(i in 2:n){
    print(i)
    if(i%%2==0){
      run_block1 <- block1(t1[i-1], lambda0[i-1], lambda1[i-1], beta[i-1], sigma.t1)
      print(run_block1)
      t1[i]<-run_block1[1]
      lambda0[i]<-run_block1[2]
      lambda1[i]<-run_block1[3]
      beta[i]<-beta[i-1] 
    }
    else{
      run_block2 <- block2(t1[i-1], lambda0[i-1], lambda1[i-1], beta[i-1], sigma.beta)
      t1[i]<-t[i-1]
      lambda0[i]<-run_block2[2]
      lambda1[i]<-run_block2[3]
      beta[i]<-run_block2[4] 
    }
   
  }
  thetamat <- data.frame(t1,lambda0, lambda1, beta)
  return(thetamat)
}

#block1(2, 2, 2, 2, 2)
#block2(2, 2, 2, 2, 2)

MCMC_block(theta0, 1000, 10,10)




```