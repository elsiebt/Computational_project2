---
title: "TMA4300 Computer Intensive Statistical Methods"
subtitle: "Exercise 2"
author: "Maja B. Mathiassen & Elsie B. Tandberg"
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes:
  - \usepackage{bm}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(boot)
library(ggplot2)
library(invgamma)
library(gridExtra)
library(MASS)
library(MCMCglmm)
```

# Problem A: The coal mining disaster data
In this problem we look at a data set of mining disasters in the UK, from March 15th 1951 to March 22nd 1962.

## A1

To get an impression of the data set, we make a plot of the cumulative number of disasters as a function of time. 
```{r A1, fig.cap="Caption",fig.width=6, fig.height=4}
#The x-axis contains the specific time of an accident
x <- coal$date

#y is the cumulative sum of disasters
y <- rep(1,length(x))
y[1]<-0
y <- cumsum(y)
y[length(x)]<-0

#Plotting
ggplot()+
  geom_line(aes(x=x[1:190],y=y[1:190])) +
  xlab('Time') + ylab('Cumulative number of disasters') + 
  ggtitle('Mining disasters') 
```

From the figure above we can see that the time between each disaster increases with time. Up until the year 1900 the cumulative number of disasters follow a linear curve, but as we reach 1900 the plot flattens indicating that there is a longer time interval between each diaster. 

## A2

We denote the starting time of the observations as $t_0$ and the end time as $t_2$. We assume the coal mining disasters follow an inhomogeneous Poisson process with intensity function

$$
\begin{aligned}
\lambda(t)=\begin{cases}
\lambda_0, \quad t\in[t_0, t_1), \\
\lambda_1, \quad t\in[t_1, t_2],
\end{cases}
\end{aligned}
$$ 

where $t_1\in(t_0, t_2)$. The likelihood function for the observed data becomes

$$
\begin{aligned}
f(x|t_1,\lambda_0,\lambda_1) = \exp\{-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)\}\lambda_0^{y_0}\lambda_1^{y_1},
\end{aligned}
$$

where $y_0$ is the number of mining disasters from $t_0$ to $t_1$, and $y_1$ is the number of disasters from $t_1$ to $t_2$. We assume $t_1\sim\text{Unif}(t_0,t_2)$ and that $t_1$, $\lambda_0$ and $\lambda_1$ all are apriori independent of each other. 

We use a gamma distributed prior for $\lambda_0$ and $\lambda_1$, with $\alpha=2$ and scale parameter $\beta$. Then

$$
\begin{aligned}
f(\lambda_i|\beta)=\frac{1}{\beta^2}\lambda_i\exp\{-\frac{\lambda_0}{\beta}\}, \quad i=0,1.
\end{aligned}
$$

For $\beta$, we use the improper prior 

$$
\begin{aligned}
f(\beta)\propto\frac{\exp\{-1/\beta\}}{\beta},
\end{aligned}
$$

for some $\beta>0$.

The parameters in our model are $\theta=(t_1,\lambda_0,\lambda_1,\beta)$. The posterior distribution of $\theta$ is given by

$$
\begin{aligned}
f(\theta|x)&=f(x|t_1, \lambda_0, \lambda_1,\beta)f(t_1)f(\lambda_0|\beta)f(\lambda_1|\beta)f(\beta)\\
&\propto\exp\{-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)\}\lambda_0^{y_0}\lambda_1^{y_1}\frac{1}{\beta^5}\lambda_0\lambda_1\exp\{-\frac{1}{\beta}(\lambda_0+\lambda_1)-\frac{1}{\beta}\}.
\end{aligned}
$$

## A3
The full conditionals are

$$
\begin{aligned}
f(t_1|...) &\propto \exp\{-t_1(\lambda_0-\lambda_1)\}\lambda_0^{y_0}\lambda_1^{y_1} \\
f(\lambda_0|...) &\propto \lambda_0^{y_0+1} \exp\{-\lambda_0(\frac{1}{\beta} +t_1-t_0)\} \\
f(\lambda_1|...) &\propto \lambda_1^{y_1+1} \exp\{-\lambda_1(\frac{1}{\beta} +t_2-t_1)\} \\
f(\beta|...) &\propto \frac{1}{\beta^5}\exp\{\frac{1}{\beta}(1+\lambda_0+\lambda_1)\}
\end{aligned}
$$
From this we can see that $f(t_1|...)$ does not belong to a distribution. Next $f(\lambda_0|...)\sim\text{Gamma}(\alpha_0,\beta_0)$ when $\alpha_0=y_0+2$ and $\beta_0^{-1}=1/\beta + t_1 - t_0$, and $f(\lambda_1|...)\sim\text{Gamma}(\alpha_1,\beta_1)$ when $\alpha_1=y_1+2$ and $\beta_1^{-1}=1/\beta + t_2 - t_1$. Lastly $f(\beta|...)\sim \text{invGamma}(4, 1+\lambda_0 + \lambda_1)$.

## A4

We want to implement a single site MCMC algorithm for $f(\theta|x)$. Since $\lambda_0$, $\lambda_1$, and $\beta$ are from known distributions we can use Gibbs sampling for these variables. For $t_1$ we choose to use random walk, and 
$$
\begin{aligned}
\tilde t_1\sim \mathcal{N}(t_1^{i-1}, \sigma^2),
\end{aligned}
$$
where $t_1^{i-1}$ is the current value and $\tilde t_1$ is the proposed new value. $y_0$ and $y_1$ are also dependent on $t_1$, so the values dependent on the current value  $t_1^{i-1}$, $y_0$ and $y_1$, and the values dependent on the proposed value, $\tilde t_1$, $\tilde y_0$ and $\tilde y_1$, are included when finding the acceptance probability. The acceptance probability is found on log scale and the expression becomes
$$
\begin{aligned}
\alpha = \min\bigg(1, \frac{f(\tilde t_1|...)}{f(t_1^{i-1}|...)}\bigg) = \min\Big(1, \exp((t_1-\tilde t_1)(\lambda_0-\lambda_1)+\log\lambda_0(\tilde y_0-y_0)+\log\lambda_1(\tilde y_1-y_1))\Big).
\end{aligned}
$$

The algorithm for the single site MCMC is now made.
```{r}
#Single site MCMC
MCMC <- function(theta, n, sigma){
  #Setting the initial values from the input
  t0 <- 1851
  t2 <- 1962
  t1 <- rep(0, n)
  t1[1]<-theta[1]
  lambda0 <- rep(0,n)
  lambda0[1] <- theta[2]
  lambda1 <- rep(0,n)
  lambda1[1]<-theta[3]
  beta <- rep(0,n)
  beta[1]<-theta[4]
  for(i in 2:n){
     #Sampling theta1 using Gibbs
      y0 <- sum(coal$date<t1[i-1])-1
      y1 <- 189-y0
      lambda0[i] <- rgamma(1, y0+2, (t1[i-1]-t0+1/beta[i-1]))
      lambda1[i] <- rgamma(1, y1+2, (t2-t1[i-1]+1/beta[i-1]))
      beta[i] <- rinvgamma(1, 4, 1+lambda0[i-1]+lambda1[i-1])
      
      
      #Metropolis-Hastings for sampling from t1
      proposal <-rnorm(1, t1[i-1], sigma) #Proposal value for t1
      y0.prop <- sum(coal$date<proposal)-1 #Corresponding proposal value for y0
      y1.prop <- 189-y0.prop #Corresponding proposal value for y1
      if(proposal<t0 || proposal>t2){ #Cheking if the proposed value lies within our predefined interval for t
        accept.prob=0
      }
      else{
        #Finding the acceptance probability
        numerator.log <- -proposal*(lambda0[i]-lambda1[i])+y0.prop*log(lambda0[i])+y1.prop*log(lambda1[i])
        denominator.log <- -t1[i-1]*(lambda0[i]-lambda1[i])+y0*log(lambda0[i])+y1*log(lambda1[i])

        accept.prob <- min(1, exp(numerator.log-denominator.log))
      }
      u <- runif(1)
      if(u<accept.prob){
        #Accept
        t1[i]<-proposal
      }
      else{
        #Reject, keep the previous value
        t1[i]<-t1[i-1]
      }
      
      }
  thetamat <- data.frame(t1,lambda0, lambda1, beta)
  return(thetamat)
}
```

## A5
We will now run our single site MCMC algorithm.
```{r}
#Defining initial values for the parameters
theta0 <- c(1900, 1, 1, 3)

#Running the single site MCMC when the tuning parameter is 3
simulation3 = MCMC(theta0,10000,3)
```

To evaluate our algorithm we will consider the burn-in and mixing properties.
First the burn-in periods are evaluated by making trace plots for each of the parameters. 
``` {r A5, fig.cap="\\label{tracesigma3} Trace plots when the tuning parameter $\\sigma$=3."}
#A function that makes the trace plots given a simulation
trace <- function(simulation){
  n = length(t(simulation[2]))
  # Trace plot of t1
  p1<-ggplot() +
    geom_line( aes(x = 1:n, y = t(simulation[1])) ) +
    xlab('Iterations') + ylab(expression(t[1])) + 
    ggtitle(expression(paste('Trace plot of ', t[1]))) 
      
  # Trace plot of lambda0
  p2<-ggplot()+
    geom_line( aes(x = 1:n, y = t(simulation[2])) ) +
    xlab('Iterations') + ylab(expression(lambda[0])) + 
    ggtitle(expression(paste('Trace plot of ', lambda[0])))
  
  # Trace plot of lambda1
  p3<-ggplot()+
    geom_line( aes(x = 1:n, y = t(simulation[3])) ) +
    xlab('Iterations') + ylab(expression(lambda[1])) + 
    ggtitle(expression(paste('Trace plot of ', lambda[1])))
  
  # Trace plot of beta
  p4<-ggplot()+
    geom_line( aes(x = 1:n, y = t(simulation[4])) ) +
    xlab('Iterations') + ylab(expression(beta)) + 
    ggtitle(expression(paste('Trace plot of ', beta))) 
  
  grid.arrange(p1, p2, p3,p4,nrow = 2)
}
#Making the trace plots for our simulation
trace(simulation3)
```

From the trace plots in Figure \ref{tracesigma3} it seems likely that the parameters have converged. The values have stabilized after about 250 iterations so this is the burn-in period for all the four parameters. For $t_1$ we see that the values do not reach the full length of the interval, $(1851, 1962)$, this is probably due to our choice of tuning parameter $sigma$ which is the standard deviation in the normal distribution $t_1$ is sampled from. It is expected to observe a wider range in values for $t_1$ for a higher value of the tuning parameter. The values for $\lambda_0$ and $\lambda_1$ also look resonable compared to the plot from Problem A1. We would expect a higher number of disaster per time up until the year 1900. As the trace plots show we would expect about 3 disaster per time ($\lambda_0$) up till $t_1$ which is estimated to be around 1890. After $t_1$ the number of disaster per time $\lambda_1$ is expected to decrase and this is what the parameter estimation for $\lambda_1$ shows as well, where the convergence lies arond 1 disaster per time. 

The mixing properties are evaluated by looking at the autocorrelation plots. 

```{r, fig.cap="\\label{autosigma3} Autocorrelation plots for $\\sigma$=3."}
#Function making the autocorrelation plots
autocorr<-function(simulation){
  par(mfrow = c(2, 2))

  #Mixing properties
  acf(simulation[1])
  acf(simulation[2])
  acf(simulation[3])
  acf(simulation[4])
}
#Running the algortihm for the autocorrelation plots
autocorr(simulation3)
```

The autocorrelations plots for  $t_1$, $\lambda_0$, $\lambda_1$ and $\beta$ in Figure \ref{autosigma3} show a decreased correlation for increased values of Lag which is good. We observe that $t_1$ have some correlation for values values that lie close but for higher Lag this correlation is negligible. 

## A6
Our algorithm has a tuning parameter $\sigma$ used to sample $t_1$ from the normal distribution. We will now explore how its value affect the burn-in and mixing properties. 


```{r, fig.cap="\\label{tracesigma2} Trace plots when the tuning parameter $\\sigma$=2.",fig.width=6, fig.height=4}
#Running the single site MCMC with tuning parameter equal to 2
simulation2<-MCMC(theta0,2000,2)
#Making the trace plots
trace(simulation2)
```

It is hard to say from Figure \ref{tracesigma2} if the parameters have converged, this is probably due to the burn-in period where we have large initial values for $\lambda_0$, $\lambda_1$ and $\beta$. From this it looks like the tuning parameters have an effect on the initial values of $\lambda_0$, $\lambda_1$ and $\beta$ since we did not have this problem when $sigma$=3.
We will now exclude the 50 first iterations and evaluate the convergnce.


```{r, fig.cap="\\label{tracesigma2.2} Trace plots when the tuning parameter $\\sigma$=2. Excluding the 50 first iterations.",fig.width=6, fig.height=4}
#Running the single site MCMC with tuning parameter equal to 2
simulation2.2<-MCMC(theta0,10000,2)[50:10000,]
#Making the trace plots
trace(simulation2.2)
```

From Figure \ref{tracesigma2.2} where some of the burn-in period is removed we observe convergence for all four parameters. We do still observe some burn-in so it is reasonable to believe that the burn-in period is about 250 iterations.We observe a more narrow range for values of $t_1$ which is expected because the tuning parameter is smaller than the one considered in A5. Apart from that the trace plots are similar to the ones from A5.The parameters have converged to reasonable values. 


```{r, fig.cap="\\label{autosigma2} Autocorrelation plots for $\\sigma$=2.",fig.width=6, fig.height=4}
#Making the autocorrelation plots for the same simulation
autocorr(simulation2)
```
```{r, fig.cap="\\label{autosigma2.2} Autocorrelation plots for $\\sigma$=2. Excluding the first 50 iterations",fig.width=6, fig.height=4}
#Making the autocorrelation plots for the same simulation
autocorr(simulation2.2)
```

From the autocorrelation plots from Figure \ref{autosigma2} and \ref{autosigma2.2} it looks like the burn-in period have an effect on the mixing, as be observe a slightly higher correlation for $t1$ in Figure \ref{autosigma2} where the burn-in period is included. In addition we oberve small mixing for $\lambda_0$ and $\lambda_1$ when the burn-in is excluded. The correlation for $t_1$ look similar to the one from Figure \ref{autosigma3}, so the change in the tuning parameter does not look like it has an effect on the autocorrelation plots.

```{r, fig.cap="\\label{tracesigma6} Trace plots when the tuning parameter $\\sigma$=6.",fig.width=6, fig.height=4}
#Running the single site MCMC with tuning parameter equal to 6
simulation6<-MCMC(theta0,10000,6)
trace(simulation6)
```

```{r, fig.cap="\\label{autosigma6} Autocorrelation plots for $\\sigma$=6.",fig.width=6, fig.height=4}
#Making the autocorrelation plots for the same simulation
autocorr(simulation6)
```

In Figure \ref{tracesigma6} and \ref{autosigma6} the tuning parameter is set to 6, we see that convergence is reached and the burn-in period is still 250 iterations. However we did not have the same problem as for $sigma=2$ where we got high initial values for the parameters. In addition we have a higher range of values for $t_1$ as expected for increased $sigma$. The mixing properties are slightly lower, we still have som correlation for $t_1$, but for the other parameters we see no correlation. 

```{r, fig.cap="\\label{tracesigma20} Trace plots when the tuning parameter $\\sigma$=20.",fig.width=6, fig.height=4}
#Running the single site MCMC with tuning parameter equal to 20
simulation20<-MCMC(theta0,10000,20)
trace(simulation20)
```

```{r, fig.cap="\\label{autosigma20} Autocorrelation plots for $\\sigma$=20.",fig.width=6, fig.height=4}
#Making the autocorrelation plots for the same simulation
autocorr(simulation20)
```
In Figure \ref{tracesigma20} and \ref{autosigma20} the tuning parameter is set to 20, we see that convergence is reached and the burn-in period is still 250 iterations. For all $t_1$ the trace plot show a wider range. The mixing properties are slightly higher for $t_1$, $\lambda_0$ and $\lambda_1$ which is not as expected from the previous aitocorrelation plots where it looks like an increased tuning parameter caused lower mixing. 

## A7
We will now implement a block Metropolis-Hastings algorithm using two block proposals. The first proposal is for $(t_1, \lambda_0, \lambda_1)$ where $\beta$ is kept unchanged. In order to generate the proposal values $(\tilde t_1, \tilde \lambda_0, \tilde \lambda_1)$, we will generate $\tilde t_1$ from a normal distribution centered at the current value of $t_1$, the same way as in A4. And then generating $(\tilde \lambda_0, \tilde \lambda_1)$ from their joint distribution, using the new value $\tilde t_1$. Then we either accept all three new values or keep the old ones.

The acceptance probability of the first block is 
$$
\alpha_1 = \min\Bigg(1,\frac{f(\tilde{t}_1, \tilde{\lambda}_0, \tilde{\lambda}_1|\beta)}{f(t_1, \lambda_0, \lambda_1|\beta)} \times \frac{Q(t_1, \lambda_0, \lambda_1|\tilde{t}_1, \tilde{\lambda}_0, \tilde{\lambda}_1, \beta))}{Q(\tilde{t}_1, \tilde{\lambda}_0, \tilde{\lambda}_1|t_1, \lambda_0, \lambda_1,\beta))} \Bigg).
$$
We note that 
$$
f(t_1, \lambda_0, \lambda_1|\beta) \propto\lambda_0^{y_0+1}\lambda_1^{y_1+1} \exp\{-\lambda_0(\frac{1}{\beta}+t_1-t_0)-\lambda_1(\frac{1}{\beta}+t_2-t_1)\},
$$
and that $f(\tilde t_1, \tilde\lambda_0,\tilde\lambda_1|\beta)$ is similar. 

The joint distribution for $(\lambda_0, \lambda_1)$ is 
$$
f(\lambda_0,\lambda_1|t_1,\beta) = \lambda_0^{y_0+1}\lambda_1^{y_1+1}\exp\{-\lambda_0(\frac{1}{\beta}+t_1-t_0)-\lambda_0(\frac{1}{\beta}+t_2-t_1)\} = f(\lambda_0|t_1,\beta)f(\lambda_1|t_1,\beta).
$$
Additionally, the proposal distribution for $\tilde t_1$ is a normal distribution only dependent on $t_1$ and a tuning parameter $\sigma_1$. Thus 
$$
\begin{aligned}
Q(\tilde{t}_1, \tilde{\lambda}_0, \tilde{\lambda}_1|t_1, \lambda_0, \lambda_1,\beta) &= f(\tilde{t}_1|t_1)f(\tilde{\lambda}_0, \tilde{\lambda}_1|\tilde{t}_1, \beta) = f(\tilde{t}_1|t_1)f(\tilde{\lambda}_0|\tilde{t}_1, \beta)f(\tilde{\lambda}_1|\tilde{t}_1, \beta) \\

&= \mathcal{N}(t_1, \sigma_1^2)\frac{(\frac{1}{\beta}+\tilde{t}_1-t_0)^{\tilde{y}_0+2}}{\Gamma(\tilde{y}_0+2)} \tilde{\lambda}_0^{\tilde{y}_0+1}\exp\{-\tilde{\lambda}_0(\frac{1}{\beta}+\tilde{t}_1-t_0)\} \\

&\quad \cdot\frac{(\frac{1}{\beta}+t_2-\tilde{t}_1)^{\tilde{y}_1+2}}{\Gamma(\tilde{y}_1+2)} \tilde{\lambda}_0^{\tilde{y}_1+1}\exp\{-\tilde{\lambda}_1(\frac{1}{\beta}+t_2-\tilde{t}_1)\}, \\

Q(t_1, \lambda_0, \lambda_1|\tilde{t}_1, \tilde{\lambda}_0, \tilde{\lambda}_1, \beta)) &= \mathcal{N}(\tilde t_1, \sigma_1^2)\frac{(\frac{1}{\beta}+t_1-t_0)^{y_0+2}}{\Gamma(y_0+2)} \lambda_0^{y_0+1}\exp\{-\lambda_0(\frac{1}{\beta}+t_1-t_0)\} \\

&\quad \cdot\frac{(\frac{1}{\beta}+t_2-t_1)^{y_1+2}}{\Gamma(y_1+2)} \lambda_0^{y_1+1}\exp\{-\lambda_1(\frac{1}{\beta}+t_2-t_1)\}.
\end{aligned}
$$
Since the normal distribution is symmetric, $\mathcal{N}(\tilde t_1,\sigma_1^2)/\mathcal{N}(t_1,\sigma_1^2)=1$. Then the acceptance probability becomes

$$
\alpha_1 = \min\Bigg(1, \frac{\Gamma(\tilde{y}_0+2)\Gamma(\tilde{y}_1+2)}{\Gamma(y_0+2)\Gamma(y_1+2)}  \frac{(\frac{1}{\beta}+t_1-t_0)^{y_0+2}(\frac{1}{\beta}+t_2-t_1)^{y_1+2}} {(\frac{1}{\beta}+\tilde{t}_1-t_0)^{\tilde{y}_0+2}(\frac{1}{\beta}+t_2-\tilde{t}_1)^{\tilde y_1+2}}\Bigg).
$$


The second block keeps $t_1$ constant. The proposed new values $(\tilde\beta, \tilde\lambda_0, \tilde\lambda_1)$ are found by first generating $\tilde\beta$ from a normal distribution centered around the old value $\beta$ and using variance $\sigma_2^2$, before finding new values $(\tilde\lambda_0,\tilde\lambda_1)$ using $\tilde\beta$. The acceptance propability for the proposed values is 

$$
\alpha_2 = \min\Bigg(1,\frac{f(\tilde{\beta}, \tilde{\lambda}_0, \tilde{\lambda}_1|t_1)}{f(\beta, \lambda_0, \lambda_1|t_1)} \times \frac{Q(\beta, \lambda_0, \lambda_1|t_1, \tilde{\lambda}_0, \tilde{\lambda}_1, \tilde\beta))}{Q(\tilde{\beta}, \tilde{\lambda}_0, \tilde{\lambda}_1|t_1, \lambda_0, \lambda_1,\beta))} \Bigg)
$$
we found that 
$$
f(\beta, \lambda_0, \lambda_1|t_1) \propto \frac{1}{\beta^5}\lambda_0^{y_0+1}\lambda_1^{y_1+1} 
\exp\{-\lambda_0(\frac{1}{\beta}+t_1-t_0)-\lambda_1(\frac{1}{\beta}+t_2-t_1)\}\exp\{-\frac{1}{\beta}\},
$$
and $f(\tilde\beta, \tilde\lambda_0,\tilde\lambda_1|t_1)$ is similar. Next we have 


$$
\begin{aligned}
Q(\tilde{\beta}, \tilde{\lambda}_0, \tilde{\lambda}_1|t_1, \lambda_0, \lambda_1,\beta) &= f(\tilde{\beta}|\beta)f(\tilde{\lambda}_0|t_1, \tilde\beta)f(\tilde{\lambda}_1|t_1, \tilde\beta) \\

&= \mathcal{N}(\beta, \sigma_2^2)\frac{(\frac{1}{\tilde\beta}+t_1-t_0)^{y_0+2}}{\Gamma(y_0+2)} \tilde{\lambda}_0^{y_0+1}\exp\{-\tilde{\lambda}_0(\frac{1}{\tilde\beta}+t_1-t_0)\} \\ 
&\quad \cdot\frac{(\frac{1}{\tilde\beta}+t_2-t_1)^{y_1+2}}{\Gamma(y_1+2)} \tilde{\lambda}_0^{y_1+1}\exp\{-\tilde{\lambda}_1(\frac{1}{\tilde\beta}+t_2-t_1)\},  \\

Q(\beta,\lambda_0,\lambda_1|t_1,\tilde\lambda_0,\tilde\lambda_1,\tilde\beta) &= \mathcal{N}(\tilde\beta, \sigma_2^2)\frac{(\frac{1}{\beta}+t_1-t_0)^{y_0+2}}{\Gamma(y_0+2)} \lambda_0^{y_0+1}\exp\{-\lambda_0(\frac{1}{\beta}+t_1-t_0)\} \\ 
&\quad \cdot\frac{(\frac{1}{\beta}+t_2-t_1)^{y_1+2}}{\Gamma(y_1+2)} \lambda_0^{y_1+1}\exp\{-\lambda_1(\frac{1}{\beta}+t_2-t_1)\}.
\end{aligned}
$$
Then the acceptance probability is 

$$
\alpha_2 = \min\Bigg(1, \frac{\beta^5}{\tilde\beta^5} \frac{(\frac{1}{\beta}+t_1-t_0)^{y_0+2}(\frac{1}{\beta}+t_2-t_1)^{y_1+2}} {(\frac{1}{\tilde\beta}+t_1-t_0)^{y_0+2}(\frac{1}{\tilde\beta}+t_2-t_1)^{y_1+2}} \frac{e^{-1/\tilde\beta}}{e^{1/\beta}}\Bigg).
$$


```{r}
#Making MCMC with block
t0 <- 1851
t2 <- 1962

block1 <- function(t1, lambda0, lambda1, beta, sigma.t1){
      #Sampling theta using Gibbs
      y0 <- sum(coal$date<t1)-1
      y1 <- 189-y0

      
      #Metropolis-Hastings for sampling from t1
      t1.prop <-rnorm(1, t1, sigma.t1) #Generating proposal value for t1 from normal distribution
      y0.prop <- sum(coal$date<t1.prop)-1
      y1.prop <- 189-y0.prop
      lambda0.prop <- rgamma(1, y0.prop+2, t1.prop-t0+1/beta) 
      lambda1.prop <- rgamma(1, y1.prop+2, t2-t1.prop+1/beta)
      

      if(t1.prop<t0 || t1.prop>t2){
        accept.prob=0
      }
      else{
        #Finding the acceptance probability
        firstpart <-(y0+2)*log(t1-t0+1/beta)+(y1+2)*log(t2-t1+1/beta)-(y0.prop+2)*log(t1.prop-t0+1/beta)-(y1.prop+2)*log(t2-t1.prop+1/beta)
        secondpart <-log((gamma(y0.prop+2)*gamma(y1.prop+2)))-log((gamma(y0+2)*gamma(y1+2)))
        sec <- sum(log(1:y0.prop+1))+sum(log(1:y1.prop+1))-sum(log(1:y0+1))-sum(log(1:y1+1))
        
      
        accept.prob <- min(1, exp(firstpart+sec))
        
      }
      u <- runif(1)
      if(u<accept.prob){
        #accept
        t1.update<-t1.prop
        lambda0.update<-lambda0.prop
        lambda1.update<-lambda1.prop
      }
      else{
        #reject, keeping old values
        t1.update<-t1
        lambda0.update<-lambda0
        lambda1.update<-lambda1
      }
      
      return(c(t1.update, lambda0.update, lambda1.update, beta))
}


```

```{r}
block2 <- function(t1, lambda0, lambda1, beta, sigma.beta){
      #Sampling theta using Gibbs
      y0 <- sum(coal$date<t1)-1
      y1 <- 189-y0

      
      #Metropolis-Hastings for sampling from t1
      beta.prop <-rtnorm(1, beta, sigma.beta, lower=0) #Generating proposal value for t1 from normal distribution
      lambda0.prop <- rgamma(1, y0+2, t1-t0+1/beta.prop) 
      lambda1.prop <- rgamma(1, y1+2, t2-t1+1/beta.prop)
      
      
      #Finding the acceptance probability
      alpha2 <- 5*log(beta)+(y0+2)*log(t1-t0+1/beta)+(y1+2)*log(t2-t1+1/beta)-5*log(beta.prop)-(y0+2)*log(t1-t0+1/beta.prop)-(y1+2)*log(t2-t1+1/beta.prop)-1/beta.prop+1/beta
      
      accept.prob2 <- min(1, exp(alpha2))
      u <- runif(1)
      if(u<accept.prob2){
        #accept
        t1<-t1
        lambda0<-lambda0.prop
        lambda1<-lambda1.prop
        beta <- beta.prop
      }
      else{
        #reject, keeping old values
        t1<-t1
        lambda0<-lambda0
        lambda1<-lambda1
        beta<- beta
      }
      
      return(c(t1, lambda0, lambda1, beta))
}
```


```{r}
MCMC_block <- function(theta, n, sigma.t1, sigma.beta){
  #Setting the initial values
  t1 <- rep(0, n)
  t1[1]<-theta[1]
  lambda0 <- rep(0,n)
  lambda0[1] <- theta[2]
  lambda1 <- rep(0,n)
  lambda1[1]<-theta[3]
  beta <- rep(0,n)
  beta[1]<-theta[4]
  
  #Generating n samples of the parameters
  for(i in 2:n){
    
    if(i%%2==0){
      run_block1 <- block1(t1[i-1], lambda0[i-1], lambda1[i-1], beta[i-1], sigma.t1)
      t1[i]<-run_block1[1]
      lambda0[i]<-run_block1[2]
      lambda1[i]<-run_block1[3]
      beta[i]<-beta[i-1] 
    }
    else{
      run_block2 <- block2(t1[i-1], lambda0[i-1], lambda1[i-1], beta[i-1], sigma.beta)
      t1[i]<-t1[i-1]
      lambda0[i]<-run_block2[2]
      lambda1[i]<-run_block2[3]
      beta[i]<-run_block2[4] 
    }
   
  }
  thetamat <- data.frame(t1,lambda0, lambda1, beta)
  return(thetamat)
}



MCMC_block(theta0,2000, 3,3 )

```

```{r, fig.cap="\\label{traceblocksigma331} Trace plots when the tuning parameters $\\sigma_{t_1}$=3 and $\\sigma_{\beta}$=3.",fig.width=6, fig.height=4}
#Running the single site MCMC with tuning parameter equal to 20
blocksimulation33<-MCMC_block(theta0,10000,3,3)
trace(blocksimulation33)
```

From the trace plots we observe high initial values for $\lambda_0$ and $\lambda_1$, this is problem might be caused by the burn-in period, and we are not able to se if it has converged for values around 3 and 1 because the y-axis is scaled to high. Trying to solve the problem by excluding the first 50 iterations below.

```{r, fig.cap="\\label{traceblocksigma332} Trace plots when the tuning parameters $\\sigma_{t_1}$=3 and $\\sigma_{\beta}$=3. Excluding the first 50 iterations.",fig.width=6, fig.height=4}
#Running the single site MCMC with tuning parameter equal to 20
blocksimulation3<-MCMC_block(theta0,10000,3,3)[50:10000,]
trace(blocksimulation3)
```

As expected Figure \ref{traceblocksigma332} shows that our parameters have converged we were only unable to see it in Figure \ref{traceblocksigma331} due to high initial values of $\lambda_0$ and $\lambda_1$. Noting that the first 50 iterations did not entirely get rid of the burn-in period, for all four parameters the burn-in period is estimated to be 250 iterations. 

```{r, fig.cap="\\label{blockautosigma33} Autocorrelation plots for Trace plots when the tuning parameters $\\sigma_{t_1}$=3 and $\\sigma_{\beta}$=3. Excluding the first 50 iterations.",fig.width=6, fig.height=4}
#Making the autocorrelation plots for the same simulation
autocorr(blocksimulation3)
```

In Figure \ref{blockautosigma33} we observe a higher mixing properties for all parameters. For $t_1$ and $\beta$ this can be explained by the way our algorithm alternaates between the two blocks. $t_1$ and $\beta$ are kept constant for every other step, so this will lead to a high correlation. For  $\lambda_0$ and $\lambda_1$ we observe higher correlation than for the single site algorithm, this might indicate that our acceptance probability is lower so that we update the values more rarely. 


```{r, fig.cap="\\label{traceblocksigma66} Trace plots when the tuning parameters $\\sigma_{t_1}$=6 and $\\sigma_{\beta}$=6.",fig.width=6, fig.height=4}
#Running the single site MCMC with tuning parameter equal to 20
blocksimulation66<-MCMC_block(theta0,10000,6,6)
trace(blocksimulation66)
```

```{r, fig.cap="\\label{blockautosigma66} Autocorrelation plots for Trace plots when the tuning parameters $\\sigma_{t_1}$=6 and $\\sigma_{\\beta}$=6.",fig.width=6, fig.height=4}
#Making the autocorrelation plots for the same simulation
autocorr(blocksimulation66)
```


In Figure \ref{traceblocksigma66} and \ref{blockautosigma66} we see the trace plots and autocorrelation plots when $\sigma_{t_1}$=6 and $\sigma_{\beta}$=6. Here we do not have the same problem with high initial values as we did for $\sigma_{t_1}$=3 and $\sigma_{\beta}$=3. This is similar to the results from Problem A6. Considering the autocorrelation plots we see that the correlation is decreased for $\beta$ and $\t_1$.


```{r, fig.cap="\\label{traceblocksigma2020} Trace plots when the tuning parameters $\\sigma_{t_1}$=20 and $\\sigma_{\beta}$=20.",fig.width=6, fig.height=4}
#Running the single site MCMC with tuning parameter equal to 20
blocksimulation2020<-MCMC_block(theta0,10000,20,20)
trace(blocksimulation2020)
```

```{r, fig.cap="\\label{blockautosigma2020} Autocorrelation plots for Trace plots when the tuning parameters $\\sigma_{t_1}$=6 and $\\sigma_{\\beta}$=6.",fig.width=6, fig.height=4}
#Making the autocorrelation plots for the same simulation
autocorr(blocksimulation2020)
```
In Figure \ref{traceblocksigma2020} and \ref{blockautosigma2020} we see the trace plots and autocorrelation plots when $\sigma_{t_1}$=20 and $\sigma_{\beta}$=20. Here we do not have the same problem with high initial values as we did for $\sigma_{t_1}$=3 and $\sigma_{\beta}$=3. This is similar to the results from Problem A6. Considering the autocorrelation plots we see that the correlation is increased for all parameters. This might indicate that we get a low acceptance probability such that most of the values are inchanged for each step. 


We will now use the simualted data to estimate the marginal posterior distributions of the parameters.

```{r, fig.cap="\\label{density} Density plot of all the estimated parameters to togheter with the theoretical distributions.",fig.width=6, fig.height=4}
x<-seq(0,4,by=0.00001)
y0 <- sum(coal$date<mean(coal$date))-1
y1 <- 189-y0
beta<-mean(blocksimulation3[,4])

#Plotting a histogram of t1 to look at the marginal posterior distribution
pp1<-ggplot()+
  geom_density(aes(x=blocksimulation3[,1],..scaled..))+
  labs(title = expression(paste("Density of ",t[1])), x=expression(t[1]))

#Plotting a histogram of lambda0 to look at the marginal posterior distribution
pp2<-ggplot()+
  geom_density(aes(x=blocksimulation3[,2],colour='Fitted'), fill="#FFCCCC")+
  geom_line(aes(x=x,y=dgamma(x,shape=y0+2,scale=1/(1/beta+mean(coal$date)-t0)), colour="Theoretical"))+
  labs(title=expression(paste("Density of ",lambda[0])),x=expression(lambda[0]))

#Plotting a histogram of lambda1 to look at the marginal posterior distribution
pp3<-ggplot()+
  geom_density(aes(x=blocksimulation3[,3], colour='Fitted'), fill="#FFCCCC")+
    geom_line(aes(x=x,y=dgamma(x,shape=y1+2,scale=1/(1/beta-mean(coal$date)+t2)), colour="Theoretical"))+
  labs(title=expression(paste("Density of ",lambda[1])),x=expression(lambda[1]))

#Plotting a histogram of beta to look at the marginal posterior distribution
pp4<-ggplot()+
  geom_density(aes(x=blocksimulation3[,4],colour='Fitted'), fill="#FFCCCC")+
  geom_line(aes(x=x,y=dinvgamma(x,shape=4,scale=1/(1+mean(blocksimulation3[,2])+mean(blocksimulation3[,3]))), colour='Theoretical'))+
  labs(title=expression(paste("Density of ",beta)),x=expression(beta))

grid.arrange(pp1, pp2, pp3, pp4, nrow = 2)

  
```

From Figure \ref{density} we see that $\lambda_0$, $\lambda_1$ and $\beta$ follow their theoretical distribution. The density of $\t_1$ is not easy to relate to a known distribution so it is hard to say if it follows a theoretical curve. However the values of $\t_1$ look reasonable as most of them are in the middle of the interval which is expected since we are sampling from a normal distribution.

```{r}
#Calculating the estimated means and covariance
meant1 <- mean(simulation3[,1])
meanlambda0 <- mean(simulation3[,2])
meanlambda1 <- mean(simulation3[,3])
meanbeta <- mean(simulation3[,4])
covlambda01 <- cov(simulation3[,2], simulation3[,3] )

meant1
meanlambda0
meanlambda1
meanbeta
covlambda01
```
We see that E[$t_1|x$]=`r meant1`, E[$\lambda_0|x$]=`r meanlambda0`, E[$\lambda_1|x$]=`r meanlambda1`, E[$\beta|x$]=`r meanbeta` and Cov[$\lambda_0, \lambda_1|x$]=`r covlambda01`. All estimated means seem reasonable as 1890 is in the middle of the time interval, and we expect more disasters before 1890, and less disasters afterwards. The covariance indicates that $\lambda_0$ and $\lambda_1$ are close to independent, which is good because this is what we have assumed earlier. 

# Problem B: INLA for Gaussian data
 
In this problem we will look at smoothing of a time series, using a simulated data set. The data set is plotted below. 
``` {r}
id = "http://www.math.ntnu.no/emner/TMA4300/2020v/Exercise/exercise2/Gaussiandata.txt"
df <- read.table(id)
n = dim(df)[1]

ggplot(df)+
  geom_point(aes(x=1:n,y=V1)) +
  xlab('t') + ylab('y') + 
  ggtitle('Gaussian data') 
```
Given a vector $\bm{\eta}=(\eta_1,...,\eta_T)$, we assume the observations $y_t$ are independent and 
$$
\begin{aligned}
y_t|\eta_t\sim\mathcal{N}(\eta_t,1), \quad t=1,...,T.
\end{aligned}
$$

We link $\eta_t$ to a smooth effect of time, such that $\eta_t=f_t$, and use a second order random walk model as a prior distribution for the vector $\bm{f}=(f_1,...,f_T)$. Then 
$$
\begin{aligned}
\pi(\bm{f}|\theta)\propto\theta^{(T-2)/2}\exp\{-\frac{\theta}{2}\sum_{t=3}^T[f_t-2f_{t-1}+f_{t-2}]^2\}=\mathcal{N}(\bm{0}, Q(\theta)^{-1}),
\end{aligned}
$$
where $Q(\theta)$ is a sparse precision matrix. Lastly we choose a gamma distribution with $\alpha=\beta=1$ as a prior for $\theta$. This is equivalent with a exponential distribution with scale parameter $1$. 


## B1

The latent field in this model is $\bm{x}=(f_1,...,f_T)$, which is Gaussian distributed with mean $\bm{0}$ and precision matrix $Q(\theta)$. Thus we have a latent Gaussian model and can use INLA.


## B2
First we note that since $y_t|\eta_t\sim\mathcal{N}(\eta_t,1)$ are independent, then $\bm{y}|\bm{\eta}\sim\mathcal{N}(\bm{\eta},I)$, where $I$ is the identity matrix.

Then we compare $\pi(\bm{f}|\theta)$ with the pdf of a Gaussian distribution, and note that $Q(\theta)=\theta A$, where $A$ is some constant matrix. Since $Q(\theta)$ is a precision matrix, it must be symmetric. Then $A$ is also symmetric, and can be written as $A=LL^T$, where $L$ is a lower triangular matrix. Thus $Q(\theta)=\theta LL^T$. By examining $\pi(\bm{f}|\theta)$, we determine that

$$
\begin{aligned}
L=\begin{pmatrix}
1 &  \\
-2 & 1 \\
1 & -2 & 1 \\
& \ddots & \ddots & \ddots \\
&& 1 & -2 & 1
\end{pmatrix}
\end{aligned}
$$
which is a $T\times(T-2)$ lower triangular matrix. 

The posterior distribution is
$$
\begin{aligned}
f(\bm{\eta},\theta|\bm{y}) &\propto f(\bm{y}|\bm{\eta},\theta)\cdot\pi(\bm{\eta}|\theta)\cdot f(\theta) \\
&\propto\exp\{-\frac{1}{2}(\bm{y-\bm{\eta})^T(\bm{y}-\bm{\eta})}\}\cdot\theta^{(T-2)/2}\exp\{-\frac{1}{2}\bm{\eta}^TQ(\theta)\bm{\eta}\} \cdot \exp\{-\theta\} \\
&= \theta^{(T-2)/2} \exp\{-\theta - \frac{1}{2}(\bm{y-\bm{\eta})^T(\bm{y}-\bm{\eta})}-\frac{1}{2}\bm{\eta}^TQ(\theta)\bm{\eta}\}.
\end{aligned}
$$

Then the full conditional of $\bm{\eta}$ becomes
$$
\begin{aligned}
f(\bm{\eta}|...) &\propto \exp\{- \frac{1}{2}(\bm{y-\bm{\eta})^T(\bm{y}-\bm{\eta})}-\frac{1}{2}\bm{\eta}^TQ(\theta)\bm{\eta}\} \\
&=\exp\{-\frac{1}{2}(\bm{y}^T\bm{y}-2\bm{y}^T\bm{\eta})+\bm{\eta}^T\bm{\eta}+\bm{\eta}^TQ(\theta)\bm{\eta}\} \\
&\propto\exp\{\bm{y}^T\bm{\eta}-\frac{1}{2}\bm{\eta}^T(Q(\theta)+I)\bm{\eta}\}
\end{aligned},
$$

which is Gaussian distributed with mean $\bm{\mu}=(Q(\theta)+I)^{-1}y$ and covariance matrix $\Sigma=(Q(\theta)+I)^{-1}$. 

The full conditional for $\theta$ is 
$$
\begin{aligned}
f(\theta|...)&\propto \theta^{(T-2)/2} \exp\{-\theta - \frac{1}{2}\bm{\eta}^TQ(\theta)\bm{\eta}\} \\
&= \theta^{(T-2)/2} \exp\{-\theta - \frac{\theta}{2}\bm{\eta}^TLL^T\bm{\eta}\} \\
&= \theta^{(T-2)/2} \exp\{-\theta(1 + \frac{1}{2}\bm{\eta}^TLL^T\bm{\eta})\}
\end{aligned}
$$

which is a Gamma distribution with $\alpha=T/2$ and $\beta^{-1}=1+1/2(\bm{\eta}^TLL^T\bm{\eta})$.

```{r}
A_matrix = function(d) {
  n = d-2
  I = diag(n)
  L1 = cbind( cbind(I, integer(n)), integer(n))
  L2 = -2 * cbind( cbind(integer(n), I), integer(n))
  L3 = cbind( cbind(integer(n), integer(n)), I)
  L = L1 + L2 + L3
  A = t(L) %*% L
  return(A)
}
```


```{r}
#Making block Gibbs algorithm
gibbs_block <- function(eta, n, iter){
  theta <- rep(1,iter)
  eta_mat <- matrix(data=NA, nrow = n, ncol=iter)
  eta_mat[,1] <- eta
  A <- A_matrix(n)
  alpha <- n/2

    for(i in 2:iter){
    beta <- 1/(t(eta_mat[,i-1])%*%A%*%eta_mat[,i-1])

    #Sampling theta from gamma distribution, gibbs step
    theta[i] <- rgamma(1, alpha, beta)
    b<- solve((theta[i]*A+diag(n)))
    eta_mat[,i] <- mvrnorm(1,b%*%df$V1, b)
    }
 
  
  return(rbind(theta, eta_mat))
}

n<-20
iter=2000
eta <- rep(3,n)
eta[n/2]<-4
samples<-gibbs_block(eta, n, iter)
samples.theta = samples[1,]
samples.eta = samples[-1,]
truehist(samples.theta)
truehist(samples.eta[10,])
```

```{r}
mean_values <- rep(0,n)
var_values <- rep(0, n)
upper <-rep(0,n)
lower <-rep(0,n)
x <-seq(1,n)
for(i in 1:n){
  mean_values[i]<-mean(samples[i+1,])
  var_values[i]<-var(samples[i+1,])
  upper[i]<-qnorm(1-0.025)*sqrt(var_values[i])+mean_values[i]
  lower[i]<-qnorm(0.025)*sqrt(var_values[i])+mean_values[i]
}
ggplot()+
  geom_line(aes(x=x, y=mean_values))+
  geom_line(aes(x=x, y=upper,colour="Upper"), linetype =3)+
  geom_line(aes(x=x, y=lower,colour="Lower"), linetype =3)+
  geom_point(aes(x=x, y=df$V1))

```
## B3

```{r}
pi_theta_given_y = function(theta_grid, y) {
  T = length(y)
  A = A_matrix(T)
  I = diag(1, T)
  z = rep(0, length(theta_grid))
  
  for ( i in 1:length(theta_grid)) {
    theta = theta_grid[i]
    Q = theta*A
    z[i] = theta^((T-2)/2) * (det(Q+I))^{-1/2} * exp(-theta) * exp( -1/2*t(y)%*%( I-solve(Q+I) )%*%y)
  }
  return(z)
}

theta = seq(0, 6, by=0.01)
y = pi_theta_given_y(theta, df$V1)
y = y / (sum(y)*0.01)
length(y)
length(theta_samples)
ggplot() + 
  geom_density(aes(x=samples.theta, colour = "Noe"), fill="#FFCCCC")+
  geom_line( aes(x=theta, y=y, colour = "Noe annet"))
#truehist(samples.theta)
```


# B4 
```{r}

pi_eta_10_given_theta_y = function(eta_grid, theta, y) {
  A = A_matrix(length(y))
  I = diag(1, length(y))
  B = solve(theta*A+I) # B = (Q+I)^(-1)
  mu = (B%*%y)[10]    
  var = B[10,10]
  pi = dnorm(eta_grid, mean = mu, sd = sqrt(var))
  return (pi)
}

pi_eta_10_given_y = function(theta_grid, eta_grid, y) {
  summ = 0
  h = eta_grid[2]-eta_grid[1] #constant step size
  theta_y = pi_theta_given_y(theta_grid, y)
  for (i in 1:length(theta_grid)) {
    summ = summ + pi_eta_10_given_theta_y(eta_grid, theta_grid[i], y) * theta_y[i] * h
  }
  return(summ)
}

theta = seq(0, 8, by=0.01)
eta = seq(-2, 2, by=0.01)
pi = pi_eta_10_given_y(theta, eta, df$V1)
pi = pi / (sum(pi)*0.01)

ggplot() + 
  geom_density(aes(x=samples.eta[10,], colour = "Noe"), fill="#FFCCCC") +
  geom_line( aes(x=eta, y=pi, colour = "Noe annet"))

```
