---
title: "TMA4300 Computer Intensive Statistical Methods"
subtitle: "Exercise 2"
author: "Maja B. Mathiassen & Elsie B. Tandberg"
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes:
  - \usepackage{bm}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(boot)
library(ggplot2)
library(invgamma)
library(gridExtra)
```

# Problem A: The coal mining disaster data
In this problem we look at a data set of mining disasters in the UK, from March 15th 1951 to March 22nd 1962.

## A1

To get an impression of the data set, we make a plot of the cumulative number of disasters as a function of time. 
```{r A1, fig.cap="Caption",fig.width=6, fig.height=4}
#The x-axis contains the specific time of an accident
x <- coal$date

#y is the cumulative sum of disasters
y <- rep(1,length(x))
y[1]<-0
y <- cumsum(y)
y[length(x)]<-0

#Plotting
ggplot()+
  geom_line(aes(x=x[1:190],y=y[1:190])) +
  xlab('Time') + ylab('Cumulative number of disasters') + 
  ggtitle('Mining disasters') 
```
From the figure above we can see that the time between each disaster increases with time. Up until the year 1900 the cumulative number of disasters follow a linear curve, but as we reach 1900 the plot flattens indicating that there is a longer time interval between each diaster. 

## A2

We denote the starting time of the observations as $t_0$ and the end time as $t_2$. We assume the coal mining disasters follow an inhomogeneous Poisson process with intensity function

$$
\begin{aligned}
\lambda(t)=\begin{cases}
\lambda_0, \quad t\in[t_0, t_1), \\
\lambda_1, \quad t\in[t_1, t_2],
\end{cases}
\end{aligned}
$$ 

where $t_1\in(t_0, t_2)$. The likelihood function for the observed data becomes

$$
\begin{aligned}
f(x|t_1,\lambda_0,\lambda_1) = \exp\{-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)\}\lambda_0^{y_0}\lambda_1^{y_1},
\end{aligned}
$$

where $y_0$ is the number of mining disasters from $t_0$ to $t_1$, and $y_1$ is the number of disasters from $t_1$ to $t_2$. We assume $t_1\sim\text{Unif}(t_0,t_2)$ and that $t_1$, $\lambda_0$ and $\lambda_1$ all are apriori independent of each other. 

We use a gamma distributed prior for $\lambda_0$ and $\lambda_1$, with $\alpha=2$ and scale parameter $\beta$. Then

$$
\begin{aligned}
f(\lambda_i|\beta)=\frac{1}{\beta^2}\lambda_i\exp\{-\frac{\lambda_0}{\beta}\}, \quad i=0,1.
\end{aligned}
$$

For $\beta$, we use the improper prior 

$$
\begin{aligned}
f(\beta)\propto\frac{\exp\{-1/\beta\}}{\beta},
\end{aligned}
$$

for some $\beta>0$.

The parameters in our model are $\theta=(t_1,\lambda_0,\lambda_1,\beta)$. The posterior distribution of $\theta$ is given by

$$
\begin{aligned}
f(\theta|x)&=f(x|t_1, \lambda_0, \lambda_1,\beta)f(t_1)f(\lambda_0|\beta)f(\lambda_1|\beta)f(\beta)\\
&\propto\exp\{-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1)\}\lambda_0^{y_0}\lambda_1^{y_1}\frac{1}{\beta^5}\lambda_0\lambda_1\exp\{-\frac{1}{\beta}(\lambda_0+\lambda_1)-\frac{1}{\beta}\}.
\end{aligned}
$$

## A3
The full conditionals are

$$
\begin{aligned}
f(t_1|...) &\propto \exp\{-t_1(\lambda_0-\lambda_1)\}\lambda_0^{y_0}\lambda_1^{y_1} \\
f(\lambda_0|...) &\propto \lambda_0^{y_0+1} \exp\{-\lambda_0(\frac{1}{\beta} +t_1-t_0)\} \\
f(\lambda_1|...) &\propto \lambda_1^{y_1+1} \exp\{-\lambda_1(\frac{1}{\beta} +t_2-t_1)\} \\
f(\beta|...) &\propto \frac{1}{\beta^5}\exp\{\frac{1}{\beta}(1+\lambda_0+\lambda_1)\}
\end{aligned}
$$
From this we can see that $f(t_1|...)$ does not belong to a distribution. Next $f(\lambda_0|...)\sim\text{Gamma}(\alpha_0,\beta_0)$ when $\alpha_0=y_0+2$ and $\beta_0^{-1}=1/\beta + t_1 - t_0$, and $f(\lambda_1|...)\sim\text{Gamma}(\alpha_1,\beta_1)$ when $\alpha_1=y_1+2$ and $\beta_1^{-1}=1/\beta + t_2 - t_1$. Lastly $f(\beta|...)\sim \text{invGamma}(4, 1+\lambda_0 + \lambda_1)$.

## A4

We want to implement a single site MCMC algorithm for $f(\theta|x)$. Since $\lambda_0$, $\lambda_1$, and $\beta$ are from known distributions we can use Gibbs sampling for these variables. For $t_1$ we choose to use random walk, and 
$$
\begin{aligned}
\tilde t_1\sim \mathcal{N}(t_1^{i-1}, \sigma^2),
\end{aligned}
$$
where $t_1^{i-1}$ is the current value and $\tilde t_1$ is the proposed new value. $y_0$ and $y_1$ are also dependent on $t_1$, so the values dependent on the current value  $t_1^{i-1}$, $y_0$ and $y_1$, and the values dependent on the proposed value, $\tilde t_1$, $\tilde y_0$ and $\tilde y_1$, are included when finding the acceptance probability. The acceptance probability is found on log scale and the expression becomes
$$
\begin{aligned}
\alpha = \min\bigg(1, \frac{f(\tilde t_1|...)}{f(t_1^{i-1}|...)}\bigg) = \min\Big(1, \exp((t_1-\tilde t_1)(\lambda_0-\lambda_1)+\log\lambda_0(\tilde y_0-y_0)+\log\lambda_1(\tilde y_1-y_1))\Big).
\end{aligned}
$$

The algortihm for the single site MCMC is now made.
```{r}
#Single site MCMC
MCMC <- function(theta, n, sigma){
  #Setting the initial values from the input
  t0 <- 1851
  t2 <- 1962
  t1 <- rep(0, n)
  t1[1]<-theta[1]
  lambda0 <- rep(0,n)
  lambda0[1] <- theta[2]
  lambda1 <- rep(0,n)
  lambda1[1]<-theta[3]
  beta <- rep(0,n)
  beta[1]<-theta[4]
  for(i in 2:n){
     #Sampling theta1 using Gibbs
      y0 <- sum(coal$date<t1[i-1])-1
      y1 <- 189-y0
      lambda0[i] <- rgamma(1, y0+2, 1/(t1[i-1]-t0+1/beta[i-1]))
      lambda1[i] <- rgamma(1, y1+2, 1/(t2-t1[i-1]+1/beta[i-1]))
      beta[i] <- rinvgamma(1, 4, 1+lambda0[i-1]+lambda1[i-1])
      
      
      #Metropolis-Hastings for sampling from t1
      proposal <-rnorm(1, t1[i-1], sigma) #Proposal value for t1
      y0.prop <- sum(coal$date<proposal)-1 #Corresponding proposal value for y0
      y1.prop <- 189-y0.prop #Corresponding proposal value for y1
      if(proposal<t0 || proposal>t2){ #Cheking if the proposed value lies within our predefined interval for t
        accept.prob=0
      }
      else{
        #Finding the acceptance probability
        numerator.log <- -proposal*(lambda0[i]-lambda1[i])+y0.prop*log(lambda0[i])+y1.prop*log(lambda1[i])
        denominator.log <- -t1[i-1]*(lambda0[i]-lambda1[i])+y0*log(lambda0[i])+y1*log(lambda1[i])

        accept.prob <- min(1, exp(numerator.log-denominator.log))
      }
      u <- runif(1)
      if(u<accept.prob){
        #Accept
        t1[i]<-proposal
      }
      else{
        #Reject, keep the previous value
        t1[i]<-t1[i-1]
      }
      
      }
  thetamat <- data.frame(t1,lambda0, lambda1, beta)
  return(thetamat)
}
```

## A5
We will now run our single site MCMC algorithm.
```{r}
#Defining initial values for the parameters
theta0 <- c(1900, 1, 1, 3)

#Running the single site MCMC when the tuning parameter is 3
simulation3 = MCMC(theta0,2000,3)
```

To evaluate our algorithm we will consider the burn-in and mixing properties.
First the burn-in periods are evaluated by making trace plots for each of the parameters. 
``` {r A5, fig.cap="\\label{tracesigma3} Trace plots when the tuning parameter $\\sigma$=3."}
#A function that makes the trace plots given a simulation
trace <- function(simulation){
  n = length(t(simulation[2]))
  # Trace plot of t1
  p1<-ggplot() +
    geom_line( aes(x = 1:n, y = t(simulation[1])) ) +
    xlab('Iterations') + ylab(expression(t[1])) + 
    ggtitle(expression(paste('Trace plot of ', t[1]))) 
      
  # Trace plot of lambda0
  p2<-ggplot()+
    geom_line( aes(x = 1:n, y = t(simulation[2])) ) +
    xlab('Iterations') + ylab(expression(lambda[0])) + 
    ggtitle(expression(paste('Trace plot of ', lambda[0])))
  
  # Trace plot of lambda1
  p3<-ggplot()+
    geom_line( aes(x = 1:n, y = t(simulation[3])) ) +
    xlab('Iterations') + ylab(expression(lambda[1])) + 
    ggtitle(expression(paste('Trace plot of ', lambda[1])))
  
  # Trace plot of beta
  p4<-ggplot()+
    geom_line( aes(x = 1:n, y = t(simulation[4])) ) +
    xlab('Iterations') + ylab(expression(beta)) + 
    ggtitle(expression(paste('Trace plot of ', beta))) 
  
  grid.arrange(p1, p2, p3,p4,nrow = 2)
}
#Making the trace plots for our simulation
trace(simulation3)
```

From the trace plots in Figure \ref{tracesigma3} it seems likely that the parameters have converged. The values have stabilized after 250 iterations so this is the burn-in period for all the four parameters. For $t_1$ we see that the values do not reach the full length of the interval, $(1851, 1962)$, this is probably due to our choice of tuning parameter $sigma$ which is the standard deviation in the normal distribution $t_1$ is sampled from. It is expected to observe a wider range in values for $t_1$ for a higher value of the tuning parameter.

The mixing properties are evaluated by looking at the autocorrelation plots. 

```{r, fig.cap="\\label{autosigma3} Autocorrelation plots for $\\sigma$=3."}
#Function making the autocorrelation plots
autocorr<-function(simulation){
  par(mfrow = c(2, 2))

  #Mixing properties
  acf(simulation[1])
  acf(simulation[2])
  acf(simulation[3])
  acf(simulation[4])
}
#Running the algortihm for the autocorrelation plots
autocorr(simulation3)
```

The autocorrelations plots for  $t_1$, $\lambda_0$, $\lambda_1$ and $\beta$ in Figure \ref{autosigma3} show a decreased correlation for increased values of Lag which is good. We observe that $t_1$, $\lambda_0$ and $\lambda_1$ have some correlation for values values that lie close but for higher Lag this correlation is negligible. 

## 6
Our algorithm has a tuning parameter $\sigma$ used to sample $t_1$ from the normal distribution. We will now explore how its value affect the burn-in and mixing properties. 


```{r, fig.cap="\\label{tracesigma2} Trace plots when the tuning parameter $\\sigma$=2.",fig.width=6, fig.height=4}
#Running the single site MCMC with tuning parameter equal to 2
simulation2<-MCMC(theta0,2000,2)
#Makig the trace plots
trace(simulation2)
```

```{r, fig.cap="\\label{autosigma2} Autocorrelation plots for $\\sigma$=2.",fig.width=6, fig.height=4}
#Making the autocorrelation plots for the same simulation
autocorr(simulation2)
```
In Figure \ref{tracesigma2} we observe a more narrow range for values of $t_1$ which is expected because the tuning parameter is smaller than the one considered in A5. Apart from that the trace plots are similar to the ones from A5, all have converged and the burn-in period is still considered to be 250 iteration. From the autocorrelation plots from Figure \ref{autosigma2} we observe a slightly lower correlation for small Lag for $\lambda_0$ and $\lambda_1$ compared to Figure \ref{autosigma3}. SI LITT MER OM MIXING!

```{r, fig.cap="\\label{tracesigma6} Trace plots when the tuning parameter $\\sigma$=6.",fig.width=6, fig.height=4}
#Running the single site MCMC with tuning parameter equal to 6
simulation6<-MCMC(theta0,2000,6)
trace(simulation6)
```

```{r, fig.cap="\\label{autosigma6} Autocorrelation plots for $\\sigma$=6.",fig.width=6, fig.height=4}
#Making the autocorrelation plots for the same simulation
autocorr(simulation6)
```

In Figure \ref{tracesigma6} and \ref{autosigma6} the tuning parameter is set to 6, we see that convergence is reached and the burn-in period is still 250 iterations. In addition we have a higher range of values for $t_1$ as expected for increased $sigma$. The mixing properties are slightly higher, we see more correlation for each value of Lag. 

```{r, fig.cap="\\label{tracesigma20} Trace plots when the tuning parameter $\\sigma$=20.",fig.width=6, fig.height=4}
#Running the single site MCMC with tuning parameter equal to 20
simulation20<-MCMC(theta0,2000,20)
trace(simulation20)
```

```{r, fig.cap="\\label{autosigma20} Autocorrelation plots for $\\sigma$=20.",fig.width=6, fig.height=4}
#Making the autocorrelation plots for the same simulation
autocorr(simulation20)
```
In Figure \ref{tracesigma20} and \ref{autosigma20} the tuning parameter is set to 20, we see that convergence is reached and the burn-in period is still 250 iterations. For all the parameters the trace plots show a wider range, and this time $t_1$ reaches the whole interval. The mixing properties are slightly higher, we see more correlation for each value of Lag. 

## 7
We will now implement a block Metropolis Hastings algortihm using two block proposals. The first proposal for $(t_1, \lambda_0, \lambda_1)$ where $\beta$ is kept unchanged. Proposal values $(\tilde t_1, \tilde \lambda_0, \tilde \lambda_1)$ will be generated by first generating $(\tilde t_1$ from the normal distribution centered at the current value of $t_1$, which was done in A4. And then generating $\tilde \lambda_0, \tilde \lambda_1)$ with the new value of $(\tilde t_1$. The acceptance probability is now changed so that it is now dependent on all the three parameters. Resulting in the following acceptance probability on log scale.

$$
\alpha=\frac{\exp(- \tilde \lambda_0(\tilde t_1-t_0)-\tilde \lambda_1(t_2-\tilde t_1)+(\tilde y_0+1)\log\tilde \lambda_0+(\tilde y_1+1)\log\tilde \lambda_1-\frac{1}{\beta}(\tilde \lambda_0+ \tilde \lambda_1))}{\exp(- \lambda_0( t_1-t_0)- \lambda_1(t_2- t_1)+(y_0+1)\log \lambda_0+(y_1+1)\log \lambda_1-\frac{1}{\beta}( \lambda_0+  \lambda_1))}
$$
When $t_1$ is kept constant we have 

$$
\alpha=\frac{\exp(- \tilde \lambda_0( t_1-t_0)-\tilde \lambda_1(t_2- t_1)+( y_0+1)\log\tilde \lambda_0+( y_1+1)\log\tilde \lambda_1-5\log\tilde \beta-\frac{1}{\tilde \beta}(\tilde \lambda_0+ \tilde \lambda_1)-\frac{1}{\tilde \beta}}{\exp(- \lambda_0( t_1-t_0)- \lambda_1(t_2- t_1)+(y_0+1)\log \lambda_0+(y_1+1)\log \lambda_1-5\log \beta-\frac{1}{\beta}( \lambda_0+  \lambda_1)-\frac{1}{\beta}}
$$

```{r}
#Making MCMC with block
theta0 <- c(1900, 1, 1, 3)
t0 <- 1851
t2 <- 1962

block1 <- function(t1, lambda0, lambda1, beta, sigma.t1){
      #Sampling theta using Gibbs
      y0 <- sum(coal$date<t1)-1
      y1 <- 189-y0

      
      #Metropolis-Hastings for sampling from t1
      t1.prop <-rnorm(1, t1, sigma.t1) #Generating proposal value for t1 from normal distribution
      y0.prop <- sum(coal$date<t1.prop)-1
      y1.prop <- 189-y0.prop
      lambda0.prop <- rgamma(1, y0.prop+2, 1/(t1.prop-t0+1/beta)) 
      lambda1.prop <- rgamma(1, y1.prop+2, 1/(t2-t1.prop+1/beta))
      

      if(t1.prop<t0 || t1.prop>t2){
        accept.prob=0
      }
      else{
        #Finding the acceptance probability
        firstpart <-exp((y0+2)*log(t1-t0+1/beta)+(y1+2)*log(t2-t1+1/beta)-((y0.prop+2)*log(t1.prop-t0+1/beta)+(y1.prop+2)*log(t2-t1.prop+1/beta)))
        secondpart <- (gamma(y0.prop+2)*gamma(y1.prop+2))/(gamma(y0+2)*gamma(y1+2))
        accept.prob <- min(1, firstpart*secondpart)
        
      }
      u <- runif(1)
      if(u<accept.prob){
        #accept
        t1.update<-t1.prop
        lambda0.update<-lambda0.prop
        lambda1.update<-lambda1.prop
      }
      else{
        #reject, keeping old values
        t1.update<-t1
        lambda0.update<-lambda0
        lambda1.update<-lambda1
      }
      
      return(c(t1.update, lambda0.update, lambda1.update, beta))
}

block1(theta0[1], theta0[2], theta0[3], theta0[4],20)

```

```{r}
block2 <- function(t1, lambda0, lambda1, beta, sigma.beta){
      #Sampling theta using Gibbs
      y0 <- sum(coal$date<t1)-1
      y1 <- 189-y0

      
      #Metropolis-Hastings for sampling from t1
      beta.prop <-rnorm(1, beta, sigma.beta) #Generating proposal value for t1 from normal distribution
      lambda0.prop <- rgamma(1, y0+2, 1/(t1-t0+1/beta.prop)) 
      lambda1.prop <- rgamma(1, y1+2, 1/(t2-t1+1/beta.prop))
      
      
      #Finding the acceptance probability
      alpha2 <- 5*log(beta)+(y0+2)*log(t1-t0+1/beta)+(y1+2)*log(t2-t1+1/beta)-5*log(beta.prop)-(y0+2)*log(t1-t0+1/beta.prop)-(y1+2)*log(t2-t1+1/beta.prop)
      print(alpha2)-1/beta.prop+1/beta
      accept.prob <- min(1, exp(alpha2))
      print(accept.prob)
      u <- runif(1)
      if(u<accept.prob){
        #accept
        lambda0<-lambda0.prop
        lambda1<-lambda1.prop
        beta <- beta.prop
      }
      else{
        #reject, keeping old values
        lambda0<-lambda0
        lambda1<-lambda1
        beta<- beta
      }
      
      return(c(t1, lambda0, lambda1, beta))
}
#block2(theta0[1], theta0[2], theta0[3], theta0[4],10)


```


```{r}
MCMC_block <- function(theta, n, sigma.t1, sigma.beta){
  #Setting the initial values
  t1 <- rep(0, n)
  t1[1]<-theta[1]
  lambda0 <- rep(0,n)
  lambda0[1] <- theta[2]
  lambda1 <- rep(0,n)
  lambda1[1]<-theta[3]
  beta <- rep(0,n)
  beta[1]<-theta[4]
  
  #Generating n samples of the parameters
  for(i in 2:n){
    print(i)
    if(i%%2==0){
      run_block1 <- block1(t1[i-1], lambda0[i-1], lambda1[i-1], beta[i-1], sigma.t1)
      print(run_block1)
      t1[i]<-run_block1[1]
      lambda0[i]<-run_block1[2]
      lambda1[i]<-run_block1[3]
      beta[i]<-beta[i-1] 
    }
    else{
      run_block2 <- block2(t1[i-1], lambda0[i-1], lambda1[i-1], beta[i-1], sigma.beta)
      t1[i]<-t[i-1]
      lambda0[i]<-run_block2[2]
      lambda1[i]<-run_block2[3]
      beta[i]<-run_block2[4] 
    }
   
  }
  thetamat <- data.frame(t1,lambda0, lambda1, beta)
  return(thetamat)
}

#block1(2, 2, 2, 2, 2)
#block2(2, 2, 2, 2, 2)

#MCMC_block(theta0, 1000, 10,10)
```







# Problem B: INLA for Gaussian data
 
In this problem we will look at smoothing of a time series, using a simulated data set. The data set is plotted below. 
``` {r}
id = "http://www.math.ntnu.no/emner/TMA4300/2020v/Exercise/exercise2/Gaussiandata.txt"
df <- read.table(id)
n = dim(df)[1]

ggplot(df)+
  geom_point(aes(x=1:n,y=V1)) +
  xlab('t') + ylab('y') + 
  ggtitle('Gaussian data') 
```
Given a vector $\bm{\eta}=(\eta_1,...,\eta_T)$, we assume the observations $y_t$ are independent and 
$$
\begin{aligned}
y_t|\eta_t\sim\mathcal{N}(\eta_t,1), \quad t=1,...,T.
\end{aligned}
$$

We link $\eta_t$ to a smooth effect of time, such that $\eta_t=f_t$, and use a second order random walk model as a prior distribution for the vector $\bm{f}=(f_1,...,f_T)$. Then 
$$
\begin{aligned}
\pi(\bm{f}|\theta)\propto\theta^{(T-2)/2}\exp\{-\frac{\theta}{2}\sum_{t=3}^T[f_t-2f_{t-1}+f_{t-2}]^2\}=\mathcal{N}(\bm{0}, Q(\theta)^{-1}),
\end{aligned}
$$
where $Q(\theta)$ is a sparse precision matrix. Lastly we choose a gamma distribution with $\alpha=\beta=1$ as a prior for $\theta$. This is equivalent with a exponential distribution with scale parameter $1$. 


## B1

The latent field in this model is $\bm{x}=(f_1,...,f_T)$, which is Gaussian distributed with mean $\bm{0}$ and precision matrix $Q(\theta)$. Thus we have a latent Gaussian model and can use INLA.


## B2
First we note that since $y_t|\eta_t\sim\mathcal{N}(\eta_t,1)$ are independent, then $\bm{y}|\bm{\eta}\sim\mathcal{N}(\bm{\eta},I)$, where $I$ is the identity matrix.

Then we compare $\pi(\bm{f}|\theta)$ with the pdf of a Gaussian distribution, and note that $Q(\theta)=\theta A$, where $A$ is some constant matrix. Since $Q(\theta)$ is a precision matrix, it must be symmetric. Then $A$ is also symmetric, and can be written as $A=LL^T$, where $L$ is a lower triangular matrix. Thus $Q(\theta)=\theta LL^T$. By examining $\pi(\bm{f}|\theta)$, we determine that

$$
\begin{aligned}
L=\begin{pmatrix}
1 &  \\
-2 & 1 \\
1 & -2 & 1 \\
& \ddots & \ddots & \ddots \\
&& 1 & -2 & 1
\end{pmatrix}
\end{aligned}
$$
which is a $T\times(T-2)$ lower triangular matrix. 

The posterior distribution is
$$
\begin{aligned}
f(\bm{\eta},\theta|\bm{y}) &\propto f(\bm{y}|\bm{\eta},\theta)\cdot\pi(\bm{\eta}|\theta)\cdot f(\theta) \\
&\propto\exp\{-\frac{1}{2}(\bm{y-\bm{\eta})^T(\bm{y}-\bm{\eta})}\}\cdot\theta^{(T-2)/2}\exp\{-\frac{1}{2}\bm{\eta}^TQ(\theta)\bm{\eta}\} \cdot \exp\{-\theta\} \\
&= \theta^{(T-2)/2} \exp\{-\theta - \frac{1}{2}(\bm{y-\bm{\eta})^T(\bm{y}-\bm{\eta})}-\frac{1}{2}\bm{\eta}^TQ(\theta)\bm{\eta}\}.
\end{aligned}
$$

Then the full conditional of $\bm{\eta}$ becomes
$$
\begin{aligned}
f(\bm{\eta}|...) &\propto \exp\{- \frac{1}{2}(\bm{y-\bm{\eta})^T(\bm{y}-\bm{\eta})}-\frac{1}{2}\bm{\eta}^TQ(\theta)\bm{\eta}\} \\
&=\exp\{-\frac{1}{2}(\bm{y}^T\bm{y}-2\bm{y}^T\bm{\eta})+\bm{\eta}^T\bm{\eta}+\bm{\eta}^TQ(\theta)\bm{\eta}\} \\
&\propto\exp\{\bm{y}^T\bm{\eta}-\frac{1}{2}\bm{\eta}^T(Q(\theta)+I)\bm{\eta}\}
\end{aligned},
$$

which is Gaussian distributed with mean $\bm{\mu}=(Q(\theta)+I)^{-1}y$ and covariance matrix $\Sigma=(Q(\theta)+I)^{-1}$. 

The full conditional for $\theta$ is 
$$
\begin{aligned}
f(\theta|...)&\propto \theta^{(T-2)/2} \exp\{-\theta - \frac{1}{2}\bm{\eta}^TQ(\theta)\bm{\eta}\} \\
&= \theta^{(T-2)/2} \exp\{-\theta - \frac{\theta}{2}\bm{\eta}^TLL^T\bm{\eta}\} \\
&= \theta^{(T-2)/2} \exp\{-\theta(1 + \frac{1}{2}\bm{\eta}^TLL^T\bm{\eta})\}
\end{aligned}
$$

which is a Gamma distribution with $\alpha=T/2$ and $\beta^{-1}=1+1/2(\bm{\eta}^TLL^T\bm{\eta})$.


